<!DOCTYPE HTML>
<html lang="zh-CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="[DL]Winograd快速卷积算法, Martin&#39;s">
    <meta name="baidu-site-verification" content>
    <meta name="google-site-verification" content>
    <meta name="360-site-verification" content>
    <meta name="description" content="前言卷积神经网络是很多任务尤其是计算机视觉任务的基础，但很大程度上，模型需要的大量卷积计算限制了模型的可用性。因此，如何快速的完成卷积操作就至关重要。
此处的卷积是指图像处理领域的卷积操作，且数据通常为多通道的二维数组，卷积核的长宽相等。
">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>[DL]Winograd快速卷积算法 | Martin&#39;s</title>
    <link rel="icon" type="image/jpeg" href="/favicon.jpg">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?46e79e71af0709a5b9106bf20cecc493";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body>

    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Martin's</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/contact" class="waves-effect waves-light">
            
            <span>留言板</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="搜索"></i>
        </a>
    </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Martin's</div>
        <div class="logo-desc">
            
            小和山职业技术学院 | 计算机科学与技术
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/contact" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                留言板
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Martin20150405" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Martin20150405" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/4.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        [DL]Winograd快速卷积算法
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                        <a href="/tags/卷积神经网络/" target="_blank">
                            <span class="chip bg-color">卷积神经网络</span>
                        </a>
                        
                        <a href="/tags/Winograd/" target="_blank">
                            <span class="chip bg-color">Winograd</span>
                        </a>
                        
                        <a href="/tags/端侧推理/" target="_blank">
                            <span class="chip bg-color">端侧推理</span>
                        </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                        <a href="/categories/机器学习/" class="post-category" target="_blank">
                            机器学习
                        </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2019-11-13
                </div>

                <div class="post-author info-break-policy">
                    <i class="fa fa-user-o fa-fw"></i>作者:&nbsp;&nbsp;
                    
                    Martin
                    
                </div>

                
                
                <div class="info-break-policy">
                    <i class="fa fa-file-word-o fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="fa fa-clock-o fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                
                

                
                <div id="busuanzi_container_page_pv" class="info-break-policy">
                    <i class="fa fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                    <span id="busuanzi_value_page_pv"></span>
                </div>
                
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>卷积神经网络是很多任务尤其是计算机视觉任务的基础，但很大程度上，模型需要的大量卷积计算限制了模型的可用性。因此，如何快速的完成卷积操作就至关重要。</p>
<p>此处的卷积是指图像处理领域的卷积操作，且数据通常为多通道的二维数组，卷积核的长宽相等。</p>
<p>常用的优化方法包括三个方面：</p>
<ul>
<li>硬件：堆数据堆模型再堆硬件是提升性能最常见的手段，本文假设读者都是买不起煤气灶的穷人</li>
<li>模型复杂度：降低模型复杂度包括降低参数冗余的花式的卷积设计，如ShuffleNet中的分组卷积+通道混合，MobileNet中的Depthwise和Pointwise分解卷积等，以及模型的裁剪、量化和稀疏化。</li>
<li>框架计算速度：目前主流的深度学习框架在加快计算上，一方面是利用数据SIMD的特性，进行硬件上并行化（SSE、Neon、线程并行），又或者通过一些矢量化手段（如Caffe、MXNet中的im2col）来充分利用软硬件的特点，实现更高的计算速度，另一方面则是像FFT、<a href="https://www.jianshu.com/p/dc67e4a3c841" target="_blank" rel="noopener">Strassen算法</a>以及Winograd算法等在卷积计算原理上不同的方法，从而减少了计算量。</li>
</ul>
<p>本文介绍的 Winograd 是存在已久最近被重新发现的方法（The Coppersmith-Winograd Matrix Multiplication Algorithm），在大部分场景中，Winograd 方法都显示和较大的优势，目前TF Lite、Tencent NCNN、Ali MNN 中计算卷积就使用了该方法。详述该方法并进行测试的是CVPR 2016中的一篇文章《Fast Algorithms for Convolutional Neural Networks》，本文主要以该文章来进行方法的介绍和讲解。</p>
<p>贴两张ARM报告的图来说明一下问题，换言之，我们的问题是需要在<strong>模型（耗时操作主要是小型卷积核如3x3的卷积）和设备确定</strong>的情况下，在<strong>Inference阶段</strong>尽可能快地计算卷积结果。</p>
<p><img src="fig_arm_1.png" alt></p>
<p><img src="fig_arm_2.png" alt></p>
<h2 id="卷积问题定义"><a href="#卷积问题定义" class="headerlink" title="卷积问题定义"></a>卷积问题定义</h2><h3 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h3><p>$Image_{H\cdot W\cdot C}$:表示一张多通道（通常为三通道的）图片，使用$I$表示，下标视情况省略，当$C$为1时，表示灰度图，当$W$为1时，表示一维数据</p>
<p>$Kernel_{R\cdot R}$：表示一个大小为$R$的二维卷积核，使用$K$表示</p>
<p>$Height/Row,Width/Col,Channel/Depth$：分别表示一张图片的行，列，通道数</p>
<p>$I_{OH\cdot OW \cdot OC}=Conv(I_{H\cdot W \cdot C},K_R)$：表示使用卷积核$K$对图像$I$进行卷积操作，得到一张新的图像，为了和论文中一致，我们也使用$F(I_{OH\cdot OW \cdot OC},K_R)$来表示上述操作。</p>
<h3 id="一维卷积"><a href="#一维卷积" class="headerlink" title="一维卷积"></a>一维卷积</h3><ul>
<li>形式:$I_{OH}=Conv(I_{H},K_R)$或$F(I_{OH},K_R)$</li>
</ul>
<p>输出的图像大小取决于卷积核大小、Stride以及Padding策略，在这里我们假设Stride都是1，没有Padding，且只统计乘法运算，则</p>
<ul>
<li>计算量：$FLO=OH\cdot R=(H-R+1)\cdot R$</li>
</ul>
<h3 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h3><ul>
<li>形式:$I_{OH\cdot OW \cdot OC}=Conv(I_{H\cdot W\cdot C},K_R)$或$F(I_{OH\cdot OW \cdot OC},K_R)$</li>
</ul>
<p>多个通道的二维卷积遵循层内连乘，层间累加的方法，需要的卷积核数量为$C\cdot OC$:</p>
<ul>
<li>计算量：$FLO=(OH\cdot OW \cdot OC)\cdot (R\cdot R)\cdot C$</li>
</ul>
<p>具体而言，输出图片的每个通道都是通过一组卷积核在所有通道上相乘累加得到的。</p>
<p><img src="fig_2d_conv.png" alt></p>
<p>有兴趣的童鞋可以自己算一下需要的加法操作次数，智障作者算了一下应该是$OP=(OH\cdot OW \cdot OC)\cdot ((R\cdot R-1)\cdot C+C-1)=(OH\cdot OW \cdot OC)\cdot (R\cdot R\cdot C-1)$次，和乘法基本相同。</p>
<p>具体讨论算法的时候我们假设输入输出的通道数都是1，即$FLO=(OH\cdot OW)\cdot (R\cdot R)$。</p>
<ul>
<li>形式:$I_{OH\cdot OW}=Conv(I_{H\cdot W},K_R)$或$F(I_{OH\cdot OW},K_R)$</li>
</ul>
<h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><blockquote>
<p>在各种开源框架中，CNN中的conv2d层执行的并不是数学上的卷积计算，而是数学上的互相关计算，具体定义的区别请参考《深度学习》Chap9.1或者<a href="https://www.zhihu.com/question/52237725/answer/545340892" target="_blank" rel="noopener">这个</a>。</p>
</blockquote>
<p><img src="fig_conv_dlbook1.png" alt></p>
<p><img src="fig_conv_dlbook2.png" alt></p>
<p>在传统的图像处理领域，卷积核的参数是<strong>已知的</strong>（且往往是对称的），而在卷积神经网络中则成为了<strong>待定参数</strong>。实际上，虽然说将卷积核上下翻转并左右翻转（即旋转180度）才是真正的卷积操作，但是也可以认为卷积层做的就是卷积，只是特征矩阵是倒序存储的。又或者说由于卷积核的参数是可变的，这样做不但提高了效率也不影响结果。</p>
<h2 id="跑题：im2col实现"><a href="#跑题：im2col实现" class="headerlink" title="跑题：im2col实现"></a>跑题：im2col实现</h2><p>im2col是一种非常容易理解的矢量化（Vectorization）手段，基于im2col和GEMM（ General Matrix Multiplication ）的方法可以获得较正常卷积计算较高的加速比，具体而言就是把我们每次进行卷积操作时涉及到的元素展开一个列向量，最终得到一个$I_{(R\cdot R)\cdot (OH\cdot OW)}$的矩阵，卷积核则拆成$K_{1\cdot (R\cdot R)}$的向量。</p>
<p><img src="fig_im2col.png" alt></p>
<p>按照同样的方法把卷积核展开，最后卷积操作就可以用一个矩阵乘法来表示，计算完成后，再使用col2im将结果转换为图片。</p>
<p><img src="fig_im2col_result.png" alt></p>
<p>这里输入是3个通道，输出是1个，相比写4个for循环，im2col<strong>大大加快了计算速度</strong>（矩阵乘法可并行，数据在内存中的存储连续，Cache命中率提高），但是<strong>没有减少计算量，且内存占用几乎倍增</strong>，另外，生成多通道矩阵依然需要写5个循环（NHWCRR），算是一种用空间来换取时间的做法。关于Caffe中im2col和col2im的实现可以看<a href="https://blog.csdn.net/jiongnima/article/details/69736844" target="_blank" rel="noopener">这里</a>，具体的实现细节其实和我们理解的过程是有较大不同的。</p>
<p><img src="fig_arm_3.png" alt></p>
<p>需要强调的一点是，由于我们的硬件设计就是良好支持并行化的，所以<strong>算得少</strong>和<strong>算得快</strong>并不能简单的认为是一回事。</p>
<p>Caffe原作者贾扬清对其框架中卷积实现的吐槽可以看<a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo" target="_blank" rel="noopener">这里</a>，摘录一段如下：</p>
<blockquote>
<p>In the last few months chatting with people about Caffe, a common comment I got was: “<em>Caffe’s convolution has some memory issues</em>.”</p>
<p>While this is true in some sense, I am not sure whether it is truly an issue - rather, <strong>it is a graduate-student level design choice when I was writing the Caffe framework in just 2 months’ budget with a looming thesis deadline.</strong> It turns out to have its own pros (faster than any trivial implementation unless you optimize really seriously) and cons (large memory consumption). A more detailed explanation follows, if you are interested.</p>
<p>……</p>
<p>Thus, I took a simpler approach: reduce the problem to a simpler one, where others have already optimized it really well. </p>
</blockquote>
<p>关于傅里叶变换计算卷积的方法在卷积神经网络中并不是很常用（因为一般还没直接算快），可以参看<a href="https://www.zhihu.com/question/264307400" target="_blank" rel="noopener">这个</a>，Strassen则是在矩阵较大时具有较好的加速效果，具体的方法细节之后再详细说</p>
<p>（因为作者也没完全看懂╮(￣▽￣)╭）。</p>
<h2 id="Winograd"><a href="#Winograd" class="headerlink" title="Winograd"></a>Winograd</h2><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>在我们的硬件受限的情况下（例如只有单核CPU），如何才能变得更高更快更强呢，由于计算一次乘法所需要的时钟周期要比计算一次加法大很多，对于乘法操作我们能够想象的下界是：输入数据的每个元素至少参与一次乘法。这也是我们力求达到的目标，即乘法次数尽可能的靠近下界。</p>
<p>形式化的表述，针对一个输出长度为$m$，卷积核大小为$r$的卷积运算，其所需要的最小乘法次数与输入数据的长度相同：</p>
<p>$$\mu(F(I_m, K_r))=m+r-1=H$$</p>
<p>每个维度相对独立，因此拓展到二维的情况也是一样的：</p>
<p>$$\begin{aligned} \mu(F(I_{m \cdot n}, K_{r \cdot s})) =\mu(F(I_{m}, K_{r})) \mu(F(I_{n}, K_{s})) =(m+r-1)(n+s-1) \end{aligned}=H \cdot W$$</p>
<p>该理论最早由<code>Shmuel Winograd</code>于1980年提出，是本文最重要的理论，没有之一。具体而言，给定一个确定的卷积问题，我们可以找到一种变换，来接近（甚至到达）乘法次数的理论下界。根据该理论，在卷积核较小的情况下，我们可以获得相对普通卷积计算而言较大的加速比。</p>
<h4 id="F-I-2-K-3-与-F-I-2-cdot-2-K-3-cdot-3"><a href="#F-I-2-K-3-与-F-I-2-cdot-2-K-3-cdot-3" class="headerlink" title="$F(I_2,K_3)$与$F(I_{2\cdot 2},K_{3 \cdot 3})$"></a>$F(I_2,K_3)$与$F(I_{2\cdot 2},K_{3 \cdot 3})$</h4><p>举个例子，$F(I_2,K_3)$表示输入信号$d=\left[ \begin{array}{llll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right]^{T}$，卷积核$g=\left[ \begin{array}{lll}{g_{0}} &amp; {g_{1}} &amp; {g_{2}}\end{array}\right]^{T}$的一维卷积操作。那么问题就可以表示为如下的形式：</p>
<p>$$F(I_2,K_3) = \left[ \begin{array}{lll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} \\ {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right] \left[ \begin{array}{l}{g_{0}} \\ {g_{1}} \\ {g_{2}}\end{array}\right]=\left[ \begin{array}{c}{r_0} \\ {r_1}\end{array}\right]$$</p>
<p> 如果是一般的矩阵乘法，则需要<strong>6次乘法和4次加法</strong>，如下： </p>
<p>$$\begin{array}{l}{r_{0}=\left(d_{0} \cdot g_{0}\right)+\left(d_{1} \cdot g_{1}\right)+\left(d_{2} \cdot g_{2}\right)} \\ {r_{1}=\left(d_{1} \cdot g_{0}\right)+\left(d_{2} \cdot g_{1}\right)+\left(d_{3} \cdot g_{2}\right)}\end{array}$$</p>
<p>但是，卷积运算中输入信号转换成的矩阵不是任意矩阵，其中<strong>有规律地分布着大量的重复元素</strong>，因此卷积转换成的矩阵乘法比一般矩阵乘法的问题域更小，这就让优化存在了可能。</p>
<p>Winograd的具体做法是， </p>
<p>$$F(I_2,K_3) = \left[ \begin{array}{lll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} \\ {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right] \left[ \begin{array}{l}{g_{0}} \\ {g_{1}} \\ {g_{2}}\end{array}\right]=\left[ \begin{array}{c}{m_{1}+m_{2}+m_{3}} \\ {m_{2}-m_{3}-m_{4}}\end{array}\right]$$</p>
<p>其中，</p>
<p>$$\begin{array}{ll}{m_{1}=\left(d_{0}-d_{2}\right) g_{0}} &amp; {m_{2}=\left(d_{1}+d_{2}\right) \frac{g_{0}+g_{1}+g_{2}}{2}} \\ {m_{4}=\left(d_{1}-d_{3}\right) g_{2}} &amp; {m_{3}=\left(d_{2}-d_{1}\right) \frac{g_{0}-g_{1}+g_{2}}{2}}\end{array}$$</p>
<p><strong>在神经网络的推理阶段，卷积核$g$上的元素是固定的</strong>，共3次加法与2次乘法，因此$g$相关的运算可以提前算好，预测阶段只需计算一次，可以忽略，所以一共所需的运算次数<strong>4次乘法和8次加法/减法</strong>，和下界一致。 速度提升比例为<strong>1.5倍</strong>。当然所需要的存储空间也相应的增加，由原来的$r$增加为$H$，即从卷积核大小变成了输入数据的宽度。</p>
<p>可是这怎么就能减少计算量呢，会不会得到的结果不一样？有兴趣的童鞋可以自行把所有符号代入化简，针对这个例子，得到的结果是<strong>完全相同</strong>的！当然在具体计算的时候可能会存在浮点精度误差。因此在这个例子里面完全就是仅仅使用不同的数学表示就达到了减少计算量的目的，真是太巧妙了。</p>
<h4 id="一维卷积形式化表述"><a href="#一维卷积形式化表述" class="headerlink" title="一维卷积形式化表述"></a>一维卷积形式化表述</h4><p>上述例子可以用通用矩阵乘法（GEMM）和元素级乘法（EWMM）的混合矩阵变换来形式化表述。</p>
<p>一维卷积的形式化表述如下：</p>
<p>$$Y=A^{T}\left[(G g) \odot\left(B^{T} d\right)\right]$$</p>
<p>详细解释如下：</p>
<ul>
<li>$g$：卷积核 ，维度$r\cdot 1$</li>
<li>$d$：输入信号，维度$H\cdot 1$</li>
<li>$G$：Filter transform矩阵，用于将卷积核变换到另一个空间，维度$(m+r-1)\cdot r=H \cdot r$</li>
<li>$B^T$：Input transform矩阵，用于将输入数据变换到另一个空间，维度$(m+r-1)\cdot (m+r-1)=H \cdot H$</li>
<li>$A^T$：Output transform矩阵，用于将数据转换回输出空间，维度$m\cdot (m+r-1)=m*H$</li>
<li>$Y$：卷积结果，维度$m\cdot 1$</li>
</ul>
<p>其中，$\odot$表示对应位置相乘，暗示此处两个矩阵的维度是一致的，上面的例子里面具体的参数如下，<br>$$<br>B^{T}=\left[ \begin{array}{cccc}{1} &amp; {0} &amp; {-1} &amp; {0} \\ {0} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {-1} &amp; {1} &amp; {0} \\ {0} &amp; {1} &amp; {0} &amp; {-1}\end{array}\right]<br>$$</p>
<p>$$<br>G=\left[ \begin{array}{ccc}{1} &amp; {0} &amp; {0} \\ {\frac{1}{2}} &amp; {\frac{1}{2}} &amp; {\frac{1}{2}} \\ {\frac{1}{2}} &amp; {-\frac{1}{2}} &amp; {\frac{1}{2}} \\ {0} &amp; {0} &amp; {1}\end{array}\right]<br>$$</p>
<p>$$<br>A^{T}=\left[ \begin{array}{llll}{1} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {1} &amp; {-1} &amp; {-1}\end{array}\right]<br>$$</p>
<p>$$<br>g=\left[ \begin{array}{lll}{g_{0}} &amp; {g_{1}} &amp; {g_{2}}\end{array}\right]^{T},d=\left[ \begin{array}{llll}{d_{0}} &amp; {d_{1}} &amp; {d_{2}} &amp; {d_{3}}\end{array}\right]^{T}<br>$$</p>
<p>整个计算过程在逻辑上可以分为4步：</p>
<ul>
<li>Input transform</li>
<li>Filter transform</li>
<li>Hadamard product（ 哈达玛积 ）</li>
<li>Output transform</li>
</ul>
<p>此处，$A^{T}$即$m$前面的系数，$B^{T}$即数据$d$前面的系数，$G$即卷积核$g$前面的系数，由于卷积核相关参数是提前计算好的，虽然我们将卷积的过程表述成了矩阵乘法的形式，<strong>但是此处只有$\odot$包含了乘法</strong>，其他的系数全是$\pm 1$，因此只有加减法。之后会提到具体如何实现，有兴趣的同学可以先自己思考一下下。</p>
<h4 id="二维卷积形式化表述"><a href="#二维卷积形式化表述" class="headerlink" title="二维卷积形式化表述"></a>二维卷积形式化表述</h4><p>文中关于一维卷积向二维卷积的扩展只有寥寥数语：</p>
<blockquote>
<p>A minimal 1D algorithm $F(m, r)$ is nested with itself to obtain a minimal 2D algorithm.</p>
<p>The nesting technique can be generalized for non-square filters and outputs,$F(m × n, r × s)$,<br>by nesting an algorithm for $F(m, r)$ with an algorithm for $F(n, s)$.</p>
</blockquote>
<p>文中给出的二维卷积形式化表述如下：<br>$$<br>Y=A^{T}\left[\left[G g G^{T}\right] \odot\left[B^{T} d B\right]\right] A<br>$$<br>详细解释如下：</p>
<ul>
<li>$g$：卷积核 ，维度$r\cdot r$</li>
<li>$d$：输入信号，维度$H\cdot H$</li>
<li>$G$：Filter transform矩阵，用于将卷积核变换到另一个空间，维度$(m+r-1)\cdot r=H \cdot r$</li>
<li>$B^T$：Input transform矩阵，用于将输入数据变换到另一个空间，维度$(m+r-1)\cdot (m+r-1)=H \cdot H$</li>
<li>$A^T$：Output transform矩阵，用于将数据转换回输出空间，维度$m\cdot (m+r-1)=m*H$</li>
<li>$Y$：卷积结果，维度$m\cdot m$</li>
</ul>
<p>对于$F(I_{2\cdot 2},K_{3 \cdot 3})$，其中所有矩阵中的参数，包括$G$，$B^T$，$A^T$都是和$F(I_2,K_3)$一样的。</p>
<p>依然只有$\odot$包含了乘法，乘法的次数为16次，相比于标准卷积的次数$36=2\times 2 \times 3 \times 3$，速度提升比例为<strong>2.25倍</strong>。其中Input transform包括$32=4\times 4 \times 2$次加法，Filter transform包括28次浮点数操作（预先计算），Output transform（Inverse transform）包括$24=2\times 2 \times 3 \times 2$次加法。</p>
<p>等等，怎么就<code>nested with itself</code>了，参数还是一样的？</p>
<p>那么问题来了：</p>
<ul>
<li>二维卷积的形式化表述是否正确，如何证明</li>
<li>通常输入的图像尺寸都较大，如何使用Winograd对其进行卷积计算，难道要实现$F(I_{448\cdot 448}, K_{3 \cdot 3})$吗，三维的卷积（卷积神经网络中的实际情况）该如何实现</li>
<li>上面形式化表示中所使用的矩阵参数该如何获得</li>
<li>形式化表示中的矩阵乘法看起来似乎比常规卷积做了更多的乘法操作，真正的算法实现是如何转换成加法的</li>
</ul>
<h4 id="二维卷积形式化推导"><a href="#二维卷积形式化推导" class="headerlink" title="二维卷积形式化推导"></a>二维卷积形式化推导</h4><p>我们先来解决第一个问题，要完成形式化的推导，我们需要先理解文章中的<code>nested with itself</code>到底是什么意思。下列图片来自ARM在Embedded Vision Summit 2018上的Slides，里面的符号表示会略有不同（ 用$k$来表示输入，$w$表示权重，$r$表示输出 ）。</p>
<p><img src="fig_arm_winograd_nest_0.png" alt></p>
<p>我们以$F(I_{2\cdot 2},K_{3 \cdot 3})$为例，输入图片是$4\times 4$的，输出是$2\times 2$的，卷积核如下：<br>$$<br>W = \left[\begin{array}{lll}{w_{0}} &amp; {w_{1}} &amp; {w_{2}} \\ {w_{3}} &amp; {w_{4}} &amp; {w_{5}} \\ {w_{6}} &amp; {w_{7}} &amp; {w_{8}}\end{array}\right]<br>$$<br>根据我们之前对标准卷积的运算次数推导，$FLO=(OH\cdot OW \cdot OC)\cdot (R\cdot R)\cdot C=36$，$OP=(OH\cdot OW \cdot OC)\cdot (R\cdot R\cdot C-1)=32$，即需要<strong>36次乘法和32次加法</strong>。</p>
<p>现在我们按照<strong>im2row</strong>的形式展开卷积流程，可以得到如下的矩阵运算：</p>
<p><img src="fig_arm_winograd_nest_1.png" alt></p>
<p>仔细观察可以发现，左侧矩阵中的部分元素是重复出现的，我们按照相同的颜色对其进行标记，并以此来进行矩阵和向量的分块操作：</p>
<p><img src="fig_arm_winograd_nest_2.png" alt>使用更加简洁的表述，我们得到了如下所示的分块运算，现在问题的表述和$F(I_2,K_3)$完全一致了，不同的是我们的每一对元素操作都是$F(I_2,K_3)$，这就是$F(I_{2\cdot 2},K_{3 \cdot 3})$的<strong>堆叠实现</strong>：</p>
<p><img src="fig_arm_winograd_nest_3.png" alt></p>
<p>形式化的表述如下：<br>$$<br>\begin{aligned} \left[\begin{array}{lll}{K_0} &amp; {K_1} &amp; {K_2} \\ {K_1} &amp; {K_2} &amp; {K_3} \end{array}\right] \left[\begin{array}{l}{W_0} \\ {W_1} \\ {W_2} \end{array}\right] &amp;= \left[\begin{array}{l}{R_0} \\ {R_1} \end{array}\right] = \left[\begin{array}{l}{K_0W_0+K_1W_1+K_2W_2} \\ {K_1W_0+K_2W_1+K_3W_2} \end{array}\right] \\ \\ &amp;= \left[\begin{array}{l}{F_{(2,3)}(D_0,W_0)+F_{(2,3)}(D_1,W_1)+F_{(2,3)}(D_2,W_2)} \\ {F_{(2,3)}(D_1,W_0)+F_{(2,3)}(D_2,W_1)+F_{(2,3)}(D_3,W_2)} \end{array}\right] \end{aligned}<br>$$<br>其中，$D_i$是$K_i$对应的输入序列，也即卷积输入的第$i$行:<br>$$<br>D = d^T=\left[\begin{array}{llll} {k_0} &amp; {k_4} &amp; {k_8} &amp; {k_{12}} \\ {k_1} &amp; {k_5} &amp; {k_9} &amp; {k_{13}} \\ {k_2} &amp; {k_6} &amp; {k_{10}} &amp; {k_{14}} \\ {k_3} &amp; {k_7} &amp; {k_{11}} &amp; {k_{15}} \end{array}\right] = \left[\begin{array}{l} D_0 &amp; D_1 &amp; D_2 &amp; D_3 \end{array}\right]<br>$$<br>我们一共使用6个$F(I_2,K_3)$来计算$F(I_{2\cdot 2},K_{3 \cdot 3})$，并额外增加了8次加法，之前提到$F(I_2,K_3)$一共所需的运算次数为4次乘法和8次加法，所以计算时期共计<strong>24次乘法与48​次加法</strong>， 速度提升比例为<strong>1.5倍</strong>。在卷积核预处理阶段，需要进行8次乘法和12次加法。</p>
<p>有兴趣的童鞋可以思考一下为什么是48次加法，因为数据有部分重复，数据变换（Input transform）只需要做4次，此处共计16次加法，而Output transform则是6次，共计24次加法，另外还有8次额外的加法（虽然只有4个加号却是8次）。</p>
<p>之前曾经提到，最小的乘法次数应该和输入的数据规模相等，因此这还不是最高的加速比（最高应该是<strong>$2.25=36\div 16$倍</strong>），还记得我们之前的一维卷积形式化表述吗，下面的图片中的所有表述和$F(I_2,K_3)$是完全一致的，但是每个元素都用矩阵和向量替换了：</p>
<p><img src="fig_arm_winograd_nest_4.png" alt></p>
<p>按照这种方式，我们可以得到$F(I_{2\cdot 2},K_{3 \cdot 3})$的<strong>嵌套实现</strong>，形式化的表述如下：<br>$$<br>\begin{aligned} \left[ \begin{array}{c}{R_0} \\ {R_1}\end{array}\right] &amp;= \left[ \begin{array}{c}{K_0 W_0 + K_1 W_1 + K_2 W_2} \\ {K_1 W_0 + K_2 W_1 + K_3 W_2} \end{array} \right] \\ \\ &amp;= \left[\begin{array}{l}{F_{(2,3)}(D_0,W_0)+F_{(2,3)}(D_1,W_1)+F_{(2,3)}(D_2,W_2)} \\ {F_{(2,3)}(D_1,W_0)+F_{(2,3)}(D_2,W_1)+F_{(2,3)}(D_3,W_2)} \end{array}\right] \\ &amp;= \left[ \begin{array}{c} {A^{T}\left[(G W_0) \odot\left(B^{T} D_0 \right)\right] + A^{T}\left[(G W_1) \odot\left(B^{T} D_1 \right)\right] + A^{T}\left[(G W_2) \odot\left(B^{T} D_2 \right)\right]} \\ {A^{T}\left[(G W_0) \odot\left(B^{T} D_1 \right)\right] + A^{T}\left[(G W_1) \odot\left(B^{T} D_2 \right)\right] + A^{T}\left[(G W_2) \odot\left(B^{T} D_3 \right)\right]} \end{array} \right] \\ \\ &amp;=A^{T}\left[\left[G [W_0 \ W_1 \ W_2 ] G^{T}\right] \odot\left[B^{T} [D_0 \ D_1 \ D_2 \ D_3] B\right]\right]A \\ \\ &amp;=A^{T}\left[\left[G w G^{T}\right] \odot\left[B^{T} d B\right]\right] A \\ \\ &amp;\textit{(…w =&gt; g…)} \\ \\ &amp;=A^{T}\left[\left[G g G^{T}\right] \odot\left[B^{T} d B\right]\right] A \end{aligned}<br>$$<br>中间的一步变化很关键，$\left[(G W_i) \odot\left(B^{T} D_j \right)\right]$是一个长度为4的列向量,$A^{T}\left[(G W_i) \odot\left(B^{T} D_j \right)\right]$则是一个长度为2的列向量，$A^{T}\left[(G W_0) \odot\left(B^{T} D_0 \right)+ (G W_1) \odot\left(B^{T} D_1 \right) + (G W_2) \odot\left(B^{T} D_2 \right)\right]$ 方括号内对应位置相乘再相加，相当于在每组相点乘结果构成的行向量上做卷积。最后的结果是一个长度为2的列向量。</p>
<p>实际上两种表述的维度并不相同，前者的维度是$I_{4\cdot 1}$，后者是$I_{2\cdot 2}$。</p>
<p>此处的推导过于复杂，作者并不会，有兴趣的童鞋可以去试一下这段代码:</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sympy <span class="token keyword">import</span> Symbol<span class="token punctuation">,</span> Matrix<span class="token punctuation">,</span>pprint<span class="token punctuation">,</span>simplify
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

BT <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
G <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
G<span class="token operator">=</span>G<span class="token operator">/</span><span class="token number">2</span>
AT <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

g <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span>f<span class="token string">'g{i}'</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
d <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span>f<span class="token string">'d{i}'</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
m <span class="token operator">=</span> Matrix<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>Symbol<span class="token punctuation">(</span>f<span class="token string">'m{i}'</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'GgGT:'</span><span class="token punctuation">)</span>
GgGT<span class="token operator">=</span>G<span class="token operator">*</span>g<span class="token operator">*</span><span class="token punctuation">(</span>G<span class="token punctuation">.</span>T<span class="token punctuation">)</span>
pprint<span class="token punctuation">(</span>GgGT<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'BTdB'</span><span class="token punctuation">)</span>
BTdB<span class="token operator">=</span>BT<span class="token operator">*</span>d<span class="token operator">*</span><span class="token punctuation">(</span>BT<span class="token punctuation">.</span>T<span class="token punctuation">)</span>
pprint<span class="token punctuation">(</span>BTdB<span class="token punctuation">)</span>

RET1<span class="token operator">=</span><span class="token punctuation">(</span>AT<span class="token operator">*</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>GgGT<span class="token punctuation">,</span>BTdB<span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>AT<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'AT * [GgGT em BTdB] * A ,shape={shape},first:'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>shape<span class="token operator">=</span>RET1<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
pprint<span class="token punctuation">(</span>simplify<span class="token punctuation">(</span>RET1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

R0<span class="token operator">=</span><span class="token punctuation">(</span>AT<span class="token operator">*</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>G<span class="token operator">*</span>g<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span>BT<span class="token operator">*</span>d<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token operator">+</span>AT<span class="token operator">*</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>G<span class="token operator">*</span>g<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span>BT<span class="token operator">*</span>d<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token operator">+</span>AT<span class="token operator">*</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>G<span class="token operator">*</span>g<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span>BT<span class="token operator">*</span>d<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span>
R1<span class="token operator">=</span><span class="token punctuation">(</span>AT<span class="token operator">*</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>G<span class="token operator">*</span>g<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span>BT<span class="token operator">*</span>d<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token operator">+</span>AT<span class="token operator">*</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>G<span class="token operator">*</span>g<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span>BT<span class="token operator">*</span>d<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token operator">+</span>AT<span class="token operator">*</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>G<span class="token operator">*</span>g<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">,</span>BT<span class="token operator">*</span>d<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'R0 ,shape={shape},first:'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>shape<span class="token operator">=</span>R0<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>
pprint<span class="token punctuation">(</span>simplify<span class="token punctuation">(</span>R0<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Diff of first:'</span><span class="token punctuation">)</span>
pprint<span class="token punctuation">(</span>simplify<span class="token punctuation">(</span>RET1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-</span>R0<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Diff of all:'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>simplify<span class="token punctuation">(</span>RET1<span class="token operator">-</span>np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span><span class="token punctuation">(</span>R0<span class="token punctuation">.</span>T<span class="token punctuation">,</span>R1<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>经过这样一番推导（虽然最关键的一步跳过了，嘻嘻嘻）我们就可以得到$F(I_{2\cdot 2},K_{3 \cdot 3})$的<strong>嵌套实现</strong>，<br>$$<br>F(I_{2\cdot 2},K_{3 \cdot 3})= A^{T} \left[ U \odot V \right] A<br>$$</p>
<p>其中，$U = G g G^{T},V = B^{T} d B$，一共需要 <strong>16次乘法和56次加法</strong>（$V = B^{T} d B$过程32次加法、$M=U \odot V$过程16次乘法、$Y=A^TMA$过程24次加法）。和一维卷积类似的，所需要的存储空间也相应的增加，由原来的$r\cdot r$增加为$H\cdot W$，即从卷积核大小变成了输入数据的宽度。</p>
<h4 id="扩展到-F-I-4-cdot-4-K-3-cdot-3"><a href="#扩展到-F-I-4-cdot-4-K-3-cdot-3" class="headerlink" title="扩展到$F(I_{4\cdot 4},K_{3 \cdot 3})$"></a>扩展到$F(I_{4\cdot 4},K_{3 \cdot 3})$</h4><p>算法本身可以被扩展到更大的输入输出尺寸，从而可能得到更高的加速比，但是参数数量和加法操作次数也相应的大大增加。<br>要计算$F(I_{4\cdot 4},K_{3 \cdot 3})$，我们的输入是$6\times 6$的图片，常规卷积的操作需要计算$144=4\times 4\times 3\times3$次乘法，但是使用Winograd只需要$36=6\times 6$次乘法，速度提升比例为<strong>4倍</strong>。具体参数如下：<br>$$<br>B^{T}=\left[\begin{array}{rrrrrr}{4} &amp; {0} &amp; {-5} &amp; {0} &amp; {1} &amp; {0} \\ {0} &amp; {-4} &amp; {-4} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {4} &amp; {-4} &amp; {-1} &amp; {1} &amp; {0} \\ {0} &amp; {-2} &amp; {-1} &amp; {2} &amp; {1} &amp; {0} \\ {0} &amp; {2} &amp; {-1} &amp; {-2} &amp; {1} &amp; {0} \\ {0} &amp; {4} &amp; {0} &amp; {-5} &amp; {0} &amp; {1}\end{array}\right]<br>$$<br>$$<br>G=\left[\begin{array}{rrr}{\frac{1}{4}} &amp; {0} &amp; {0} \\ {-\frac{1}{6}} &amp; {-\frac{1}{6}} &amp; {-\frac{1}{6}} \\ {-\frac{1}{6}} &amp; {\frac{1}{6}} &amp; {-\frac{1}{6}} \\ {\frac{1}{24}} &amp; {\frac{1}{12}} &amp; {\frac{1}{6}} \\ {\frac{1}{24}} &amp; {-\frac{1}{12}} &amp; {\frac{1}{6}} \\ {0} &amp; {0} &amp; {1}\end{array}\right]<br>$$<br>$$<br>A^{T}=\left[\begin{array}{rrrrrr}{1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {0} \\ {0} &amp; {1} &amp; {-1} &amp; {2} &amp; {-2} &amp; {0} \\ {0} &amp; {1} &amp; {1} &amp; {4} &amp; {4} &amp; {0} \\ {0} &amp; {1} &amp; {-1} &amp; {8} &amp; {-8} &amp; {1}\end{array}\right]<br>$$</p>
<blockquote>
<p>The number of additions and constant multiplications required by the minimal Winograd transforms increases quadratically with the tile size. Thus for large tiles, the complexity of the transforms will overwhelm any savings in the number of multiplications.</p>
<p>The magnitude of the transform matrix elements also increases with increasing tile size. This effectively reduces the numeric accuracy of the computation, so that for large tiles, the transforms cannot be computed accurately.</p>
</blockquote>
<p>但是浮点操作次数也大大增加了，其中Input transform包括$144=12\times (6+6)$次浮点数操作（注意此时的系数已经不全是1了，因此也会包括乘法），Filter transform包括72次浮点数操作（预先计算），Output transform（Inverse transform）包括$100=10\times (6+4)$次浮点数操作。</p>
<p>此外，转换矩阵的规模增大导致了计算精度误差增加，不过作者认为卷积神经网络对精度的要求其实比较低，因此在附录中讨论了$F(I_{6\cdot 6},K_{3 \cdot 3})$的可能性，其参数如下：<br>$$<br>B^{T}=\left[\begin{array}{rrrrrrrr}   {1} &amp; {0} &amp; {-21/4} &amp; {0} &amp; {21/4} &amp; {0} &amp; {-1} &amp; {0} \\   {0} &amp; {1} &amp; {1} &amp; {-17/4} &amp; {-17/4} &amp; {1} &amp; {1} &amp; {0} \\   {0} &amp; {-1} &amp; {1} &amp; {17/4} &amp; {-17/4} &amp; {-1} &amp; {1} &amp; {0} \\   {0} &amp; {1/2} &amp; {1/4} &amp; {-5/2} &amp; {-5/4} &amp; {2} &amp; {1} &amp; {0} \\   {0} &amp; {-1/2} &amp; {1/4} &amp; {5/2} &amp; {-5/4} &amp; {-2} &amp; {1} &amp; {0} \\   {0} &amp; {2} &amp; {4} &amp; {-5/2} &amp; {-5} &amp; {1/2} &amp; {1} &amp; {0} \\   {0} &amp; {-2} &amp; {4} &amp; {5/2} &amp; {-5} &amp; {-1/2} &amp; {1} &amp; {0} \\   {0} &amp; {-1} &amp; {0} &amp; {21/4} &amp; {0} &amp; {-21/4} &amp; {0} &amp; {1}   \end{array}\right],   \\   G=\left[\begin{array}{rrr}   {1} &amp; {0} &amp; {0} \\   {-2/9} &amp; {-2/9} &amp; {-2/9} \\   {-2/9} &amp; {2/9} &amp; {-2/9} \\   {1/90} &amp; {1/45} &amp; {2/45} \\   {1/90} &amp; {-1/45} &amp; {2/45} \\   {32/45} &amp; {16/45} &amp; {8/45} \\   {32/45} &amp; {-16/45} &amp; {8/45} \\   {0} &amp; {0} &amp; {1}   \end{array}\right],   \\   A^{T}=\left[\begin{array}{rrrrrrrr}   {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {1} &amp; {0} \\   {0} &amp; {1} &amp; {-1} &amp; {2} &amp; {-2} &amp; {1/2} &amp; {-1/2} &amp; {0} \\   {0} &amp; {1} &amp; {1} &amp; {4} &amp; {4} &amp; {1/4} &amp; {1/4} &amp; {0} \\   {0} &amp; {1} &amp; {-1} &amp; {8} &amp; {-8} &amp; {1/8} &amp; {-1/8} &amp; {0} \\   {0} &amp; {1} &amp; {1} &amp; {16} &amp; {16} &amp; {1/16} &amp; {1/16} &amp; {0} \\   {0} &amp; {1} &amp; {-1} &amp; {32} &amp; {-32} &amp; {1/32} &amp; {-1/32} &amp; {1} \\   \end{array}\right]<br>$$<br>相比直接卷积324次的乘法操作，$F(I_{6\cdot 6},K_{3 \cdot 3})$只需要64次，加速比达到<strong>5.06倍</strong>（然而并没有）。我们目前讨论的都是$3\times 3$的卷积，只是<code>tile</code>大小不同。</p>
<h3 id="基于Winograd的卷积计算算法流程"><a href="#基于Winograd的卷积计算算法流程" class="headerlink" title="基于Winograd的卷积计算算法流程"></a>基于Winograd的卷积计算算法流程</h3><p>第二个问题，其实我们并不会真的去实现$F(I_{448\cdot 448}, K_{3 \cdot 3})$，而是将图片划分成多个相同大小部分重叠的<code>tile</code>，在此基础上使用如$F(I_{2\cdot 2},K_{3 \cdot 3})$的方式来计算，最后合并统计结果。对于三维卷积，实际上是和标准卷积一样，逐层做二维卷积，再每层对应位置结果相加。但除此之外，针对多个卷积核还有更加巧妙的做法。</p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p><code>tile</code>在此处代表一个图像块，具体划分方式如下：</p>
<p>下图展示了标准卷积和Winograd $F(I_{2\cdot 2},K_{3 \cdot 3})$的区别,标准的卷积过程： ：</p>
<p><img src="fig_standard_conv.gif" alt></p>
<p>Winograd $F(I_{2\cdot 2},K_{3 \cdot 3})$:</p>
<p><img src="fig_winograd_conv.gif" alt></p>
<blockquote>
<p>前述只讨论了一些比较简单的情况，事实上在CNN中，由于输入的特征图只需要变换一次，而却会被多个滤波器复用，所以输入变换过程的额外开销会被平摊——卷积的滤波器（也即输出通道）越多，那么输入变换产生的额外开销的影响就越小。 </p>
</blockquote>
<h4 id="多卷积核"><a href="#多卷积核" class="headerlink" title="多卷积核"></a>多卷积核</h4><h3 id="参数推导（劝退节）"><a href="#参数推导（劝退节）" class="headerlink" title="参数推导（劝退节）"></a>参数推导（劝退节）</h3><h4 id="欧几里得定理（辗转相除法）"><a href="#欧几里得定理（辗转相除法）" class="headerlink" title="欧几里得定理（辗转相除法）"></a>欧几里得定理（辗转相除法）</h4><h4 id="贝祖定理"><a href="#贝祖定理" class="headerlink" title="贝祖定理"></a>贝祖定理</h4><h4 id="扩展欧几里得定理"><a href="#扩展欧几里得定理" class="headerlink" title="扩展欧几里得定理"></a>扩展欧几里得定理</h4><h4 id="中国剩余定理CRT"><a href="#中国剩余定理CRT" class="headerlink" title="中国剩余定理CRT"></a>中国剩余定理CRT</h4><h4 id="拉格朗日插值法"><a href="#拉格朗日插值法" class="headerlink" title="拉格朗日插值法"></a>拉格朗日插值法</h4><h4 id="Cook-Toom-algorithm"><a href="#Cook-Toom-algorithm" class="headerlink" title="Cook-Toom algorithm"></a>Cook-Toom algorithm</h4><h4 id="Winograd-Algorithm"><a href="#Winograd-Algorithm" class="headerlink" title="Winograd Algorithm"></a>Winograd Algorithm</h4><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="计算优化"><a href="#计算优化" class="headerlink" title="计算优化"></a>计算优化</h4><h4 id="算法复杂度分析"><a href="#算法复杂度分析" class="headerlink" title="算法复杂度分析"></a>算法复杂度分析</h4><h4 id="实验设定与结果"><a href="#实验设定与结果" class="headerlink" title="实验设定与结果"></a>实验设定与结果</h4><h4 id="内存占用、优化技巧与加速比"><a href="#内存占用、优化技巧与加速比" class="headerlink" title="内存占用、优化技巧与加速比"></a>内存占用、优化技巧与加速比</h4><p>$F(I_{2\cdot 2},K_{3 \cdot 3})$的加速比上限是2.25，$F(I_{4\cdot 4},K_{3 \cdot 3})$的加速比上限是4。</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="矩阵参数获取"><a href="#矩阵参数获取" class="headerlink" title="矩阵参数获取"></a>矩阵参数获取</h4><h4 id="端侧推理"><a href="#端侧推理" class="headerlink" title="端侧推理"></a>端侧推理</h4><p>以$F(I_{2\cdot 2},K_{3 \cdot 3})$为例，我们来看看代码是如何实现的，代码来自Tencent NCNN。</p>
<ol>
<li>Filter Transform</li>
<li>Input transform</li>
<li>Hadamard product</li>
<li>Output transform</li>
</ol>
<p>$F(I_{4\cdot 4},K_{3 \cdot 3})$的实现就更加复杂，可以看参考资料中Tencent NCNN的实现。</p>
<h2 id="跑题：-NHWC、HCHW和NC-4HW4"><a href="#跑题：-NHWC、HCHW和NC-4HW4" class="headerlink" title="跑题： NHWC、HCHW和NC/4HW4"></a>跑题： NHWC、HCHW和NC/4HW4</h2><p>我们在说NHWC时，实际上是说数据在内存上的排布策略，以及探讨该策略带来的访存性能变化。</p>
<p>NHWC与HCHW的区别用一张图就可以说清楚，其中我们的N=1，C=3，H=1，W=6：</p>
<p><img src="fig_nchw.webp" alt></p>
<blockquote>
<p><strong>NHWC 的访存局部性更好</strong>（每三个输入像素即可得到一个输出像素），<strong>NCHW</strong> 则必须等所有通道输入准备好才能得到最终输出结果，<strong>需要占用较大的临时空间</strong>。</p>
<p>在 CNN 中常常见到 1x1 卷积（例如：<a href="http://mp.weixin.qq.com/s?__biz=MzI2MzYwNzUyNg==&mid=2247483973&idx=1&sn=b0b9aa4190f5ac9a34421beaa92eb932&chksm=eab807ccddcf8edaa798098c73b82ee35f4b22e159ddcd4ffb0d0cd6cae77a170a59a5c441e4&scene=21#wechat_redirect" target="_blank" rel="noopener">用于移动和嵌入式视觉应用的 MobileNets</a>），也是每个输入 channel 乘一个权值，然后将所有 channel 结果累加得到一个输出 channel。如果使用 NHWC 数据格式，可以将卷积计算简化为矩阵乘计算，即 <strong>1x1 卷积核实现了每个输入像素组到每个输出像素组的线性变换</strong>。</p>
<p>TensorFlow 为什么选择 NHWC 格式作为默认格式？因为早期开发都是基于 CPU，使用 NHWC 比 NCHW 稍快一些（不难理解，NHWC 局部性更好，cache 利用率高）。</p>
<p>NCHW 则是 Nvidia cuDNN 默认格式，使用 GPU 加速时用 NCHW 格式速度会更快（也有个别情况例外）。</p>
</blockquote>
<p>最早接触到NC/4HW4是因为阿里的端侧推理框架MNN，不得不说这个框架真是太复杂了，随便拿一个技术点就能扯上半天。</p>
<p>NC/4HW4第一个4表示把原Feature map的通道按4分组不够补0，然后每组内的4个Feature map按照RGBA交织排列。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>Winograd算法存在一定的计算精度损失。不过CNN模型需要的计算精度实际上很低，例如有用fp16、int8实现CNN的方法，也有用更低bit数甚至binary计算实现的方法，它们都有不错的ImageNet分类精度。 </li>
<li>Winograd算法可以用矩阵形式来表示，但是具体实现时，并不意味着要调用矩阵运算的接口，为了更快的计算速度，通常会直接将计算展开，故代码量较大，且对于不同的<code>tile</code>大小需要专门定制的代码（好在也就那么几种），通常卷积核的大小为$2\times 2$到$7\times 7$。</li>
<li></li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://www.cnblogs.com/shine-lee/p/10906535.html" target="_blank" rel="noopener">卷积神经网络中的Winograd快速卷积算法</a></li>
<li><a href="https://hey-yahei.cn/2019/08/21/winograd_convolution/" target="_blank" rel="noopener">Winograd卷积原理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/82482351" target="_blank" rel="noopener">源于《孙子算经》的Cudnn</a></li>
<li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lavin_Fast_Algorithms_for_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Paper-CVPR 2016-Fast Algorithms for Convolutional Neural Networks</a></li>
<li><a href="https://books.google.com/books?id=wANiW8bGQpEC" target="_blank" rel="noopener">Book-Shmuel Winograd.1980 Arithmetic complexity of computations</a></li>
<li><a href="https://yq.aliyun.com/articles/707074" target="_blank" rel="noopener">开源背后 | 面对端侧推理引擎的挑战，阿里工程师如何应对</a></li>
<li><a href="https://github.com/Tencent/FeatherCNN/blob/booster/src/booster/arm/winograd_kernels.cpp" target="_blank" rel="noopener">Tencent FeatherCNN实现</a></li>
<li><a href="https://github.com/Tencent/ncnn/blob/master/src/layer/x86/convolution_3x3.h" target="_blank" rel="noopener">Tencent NCNN 3x3实现</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/51569443" target="_blank" rel="noopener">深度学习轻量级推理及加速</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/67117914" target="_blank" rel="noopener">【移动端DL框架】当前主流的移动端深度学习框架一览</a></li>
<li><a href="https://www.bilibili.com/video/av50718398" target="_blank" rel="noopener">Video-Fast Algorithms for Convolutional Neural Networks by Andrew Lavin and Scott Gray</a></li>
<li><a href="https://www.bilibili.com/video/av53072685" target="_blank" rel="noopener">Video-Even Faster CNNs Exploring the New Class of Winograd Algorithms</a></li>
<li><a href="https://www.slideshare.net/embeddedvision/even-faster-cnns-exploring-the-new-class-of-winograd-algorithms-a-presentation-from-arm?from_action=save" target="_blank" rel="noopener">PPT-算法解析</a></li>
<li><a href="https://arxiv.org/abs/1810.01973" target="_blank" rel="noopener">Paper-Sparse Winograd Convolutional neural networks on small-scale systolic arrays</a> </li>
<li><a href="https://github.com/andravin/wincnn" target="_blank" rel="noopener">Code-参数生成 andravin/wincnn</a></li>
<li><a href="http://people.ece.umn.edu/users/parhi/SLIDES/chap8.pdf" target="_blank" rel="noopener">PPT-Fast Convolution</a></li>
</ul>

            </div>
            <hr />

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">写作不易，客官能否打赏一杯奶茶？</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.png" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            

    <div class="reprint" id="reprint-statement">
        <p class="reprint-tip">
            <i class="fa fa-exclamation-triangle"></i>&nbsp;&nbsp;
            <span>转载规则</span>
        </p>
        
            <div class="center-align">
                <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                    <img alt=""
                         style="border-width:0"
                         src="https://i.creativecommons.org/l/by/4.0/88x31.png"/>
                </a>
            </div>
            <br/>
            <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text"
                  property="dct:title" rel="dct:type">
                    《[DL]Winograd快速卷积算法》
                </span> 由
            <a xmlns:cc="http://creativecommons.org/ns#" href="/2019/11/13/dl-winograd-kuai-su-juan-ji-suan-fa/" property="cc:attributionName"
               rel="cc:attributionURL">
                Martin
            </a> 采用
            <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                知识共享署名 4.0 国际许可协议
            </a>进行许可。
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>


        </div>
    </div>

    
    <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: 'deca0b6b8d29ead85fc3',
        clientSecret: '73935e45becc6a90b8dc34e03059f7f8ef8815f3',
        repo: 'martin20150405.github.io',
        owner: 'martin20150405',
        admin: "martin20150405",
        id: '2019/11/13/dl-winograd-kuai-su-juan-ji-suan-fa/',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-dot-circle-o"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/2019/11/13/dl-winograd-kuai-su-juan-ji-suan-fa/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/4.jpg" class="responsive-img" alt="[DL]Winograd快速卷积算法">
                        
                        <span class="card-title">[DL]Winograd快速卷积算法</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            前言卷积神经网络是很多任务尤其是计算机视觉任务的基础，但很大程度上，模型需要的大量卷积计算限制了模型的可用性。因此，如何快速的完成卷积操作就至关重要。
此处的卷积是指图像处理领域的卷积操作，且数据通常为多通道的二维数组，卷积核的长宽相等。

                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2019-11-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/机器学习/" class="post-category" target="_blank">
                                    机器学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/卷积神经网络/" target="_blank">
                        <span class="chip bg-color">卷积神经网络</span>
                    </a>
                    
                    <a href="/tags/Winograd/" target="_blank">
                        <span class="chip bg-color">Winograd</span>
                    </a>
                    
                    <a href="/tags/端侧推理/" target="_blank">
                        <span class="chip bg-color">端侧推理</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2019/11/09/dong-tai-gui-hua-leetcode-72-bian-ji-ju-chi/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/7.jpg" class="responsive-img" alt="[动态规划]LeetCode-72.编辑距离">
                        
                        <span class="card-title">[动态规划]LeetCode-72.编辑距离</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            题目给定两个单词 word1 和 word2，计算出将 word1 转换成 word2 所使用的最少操作数 。
你可以对一个单词进行如下三种操作：

插入一个字符

删除一个字符

替换一个字符


示例 1:
输入: word1 = &q
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2019-11-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/题解/" class="post-category" target="_blank">
                                    题解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/动态规划/" target="_blank">
                        <span class="chip bg-color">动态规划</span>
                    </a>
                    
                    <a href="/tags/经典算法题/" target="_blank">
                        <span class="chip bg-color">经典算法题</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Martin's<br />'
            + '作者: Martin<br />'
            + '链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () { bodyElement.removeChild(newdiv); }, 200);
    });
</script>

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>

    <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            &copy; 2019 Martin. All Rights Reserved.

            
            &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
            <span class="white-color">10.4k</span>
            

            <br>
            <span id="sitetime"></span>

            
            
            <br>
            
            <span id="busuanzi_container_site_pv" style='display:none'>
                <i class="fa fa-heart-o"></i>
                本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
            <span id="busuanzi_container_site_uv" style='display:none'>
                人次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人.
            </span>
            
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Martin20150405" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:martin20150405@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="https://zhihu.com/people/martin20150405" class="tooltipped" target="_blank" data-tooltip="访问我的知乎" data-position="top" data-delay="50">
        <i class="fa fa-inverse">知</i>
    </a>



    <a href="https://user.qzone.qq.com/." class="tooltipped" target="_blank" data-tooltip="访问我的QQ空间" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="https://weibo.com/." class="tooltipped" target="_blank" data-tooltip="关注我的微博" data-position="top" data-delay="50">
        <i class="fa fa-weibo"></i>
    </a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>
</div>
    </div>
</footer>

<div class="progress-bar"></div>

<!-- 不蒜子计数初始值纠正 -->
<script>
    $(document).ready(function () {

        var int = setInterval(fixCount, 50);
        var pvcountOffset = 80000;
        var uvcountOffset = 20000;

        function fixCount() {
            if (document.getElementById("busuanzi_container_site_pv").style.display != "none") {
                $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + pvcountOffset);
                clearInterval(int);
            }
            if ($("#busuanzi_container_site_pv").css("display") != "none") {
                $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + uvcountOffset); // 加上初始数据 
                clearInterval(int);
            }
        }
    });
</script>

<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */
        var t1 = Date.UTC(2019, 11, 1, 00, 00, 00); 
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "本站已运行 " + diffYears + " 年 " + diffDays + " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
    }
    siteTime();
</script>

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- <script type="text/javascript"> var OriginTitile = document.title, st; document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ喔哟，崩溃啦！", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) -->
    </script>

    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', '');
</script>



    

    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    <!-- 雪花特效 -->
    
	
    
    <script type="text/javascript" color="0,0,255"
        pointColor="0,0,255" opacity='0.7'
        zIndex="-1" count="99"
        src="/libs/background/canvas-nest.js"></script>
    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    
</body>

</html>